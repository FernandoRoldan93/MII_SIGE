---
title: "Actividad2"
output:
  pdf_document: default
  html_document:
    df_print: paged
html_notebook: default
---

## Enunciado del ejercicio
Crea un cuaderno .Rmd a partir de titanic.Rmd (sección Predicción automática) que realice las siguientes tareas empleando caret:

  1. Aprendizaje de modelo de clasificación utilizando 'Random Forest' (modelo_rf1)

  2. Variación sobre modelo1 utilizando una partición de datos 80-20 y validación cruzada (modelo_rf2)

  3. Comparación y selección del mejor modelo en modelo_rf1, modelo_rf2 términos de ROC y AUC (modelo_rf)X

  4. Aprendizaje de modelo de clasificación utilizando redes neuronales - perceptrón multicapa y parámetros por defecto (modelo_rna)

  5. Envío de solución a Kaggle con modelo_rna

  6. Mejora de modelo_rna mediante entrenamiento con rejilla de parámetros para .size, .decay (modelo_rna_mejorado)

  7. Envío de solución a Kaggle con modelo_rna_mejorado

  8. Comparación de modelo_rna_mejorado con modelo_rf

Sube el cuaderno, indicando al final del mismo en una tabla las métricas obtenidas para modelo_rf y modelo_rna_mejorado.




## Lectura de datos y preprocesamiento

En esta sección se leeran y preprocesaran los datos tal y como se realiza en el fichero de ejemplo de "[Titanic.Rmd](https://github.com/jgromero/sige2020/blob/master/Teor%C3%ADa/03%20An%C3%A1lisis%20predictivo/code/titanic.Rmd)", aportado por el profesor.

```{r}
library(tidyverse)
data_raw <- read_csv('train.csv')
data_raw # str(data_raw) , glimpse(data_raw)

#Valores perdidos
library(funModeling)
df_status(data_raw)

```

Reducimos el conjunto de datos eliminando las variables que no son utiles

```{r}

status <- df_status(data_raw)
## columnas con NAs
na_cols <- status %>%
  filter(p_na > 70) %>%
  select(variable)
## columnas con valores diferentes
dif_cols <- status %>%
  filter(unique > 0.8 * nrow(data_raw)) %>%
  select(variable)
## eliminar columnas
remove_cols <- bind_rows(
  list(na_cols, dif_cols)
)
data_reduced <- data_raw %>%
  select(-one_of(remove_cols$variable))

library(caret)
data <-
  data_raw %>%
  mutate(Survived = as.factor(ifelse(Survived == 1, 'Yes', 'No'))) %>%
  mutate(Pclass = as.factor(Pclass)) %>%
  mutate(Fare_Interval = as.factor(
    case_when(
      Fare >= 30 ~ 'More.than.30',
      Fare >= 20 & Fare < 30 ~ 'Between.20.30',
      Fare < 20 & Fare >= 10 ~ 'Between.10.20',
      Fare < 10 ~ 'Less.than.10'))) %>%
  select(Survived, Pclass, Sex, Fare_Interval)



```

## Random Forest (modelo_rf1)

Primero preparamos el modelo como en el fichero aportado por el profesor:

```{r}

rpartCtrl <- trainControl(classProbs = TRUE, summaryFunction = twoClassSummary)
trainIndex <- createDataPartition(data$Survived, p = 1, list = FALSE)
train <- data[trainIndex, ]
```

Entrenamos el modelo random forest

```{r}
set.seed(1)
rf_random1 <- train(Survived ~ .,
                   data = train,
                   method = 'rf',
                   metric = 'ROC',
                  trControl = rpartCtrl)
print(rf_random1)

```
validación

```{r}
prediction <- predict(rf_random1, train, type = "raw") 
cm_train <- confusionMatrix(prediction, train[["Survived"]])
cm_train
```

Calculamos la curva ROC

```{r}
library(pROC)
predictionValidationProb <- predict(rf_random1, train, type = "prob")
auc1 <- roc(train$Survived,                       # columna 'target' del conjunto de entrenamiento
           predictionValidationProb[["Yes"]])  # columna de predicciones de la clase positiva como probabilidades
roc_validation1 <- plot.roc(auc1, 
                           ylim=c(0,1), 
                           type = "S" , 
                           print.thres = TRUE, 
                           main=paste('Validation AUC:', round(auc1$auc[[1]], 2)))
```





## Random forest con particion de datos

```{r}

rpartCtrl <- trainControl(classProbs = TRUE, summaryFunction = twoClassSummary)
trainIndex <- createDataPartition(data$Survived, p = .8, list = FALSE)
train <- data[trainIndex, ] 
val   <- data[-trainIndex, ]


rf_random2 <- train(Survived ~ .,
                   data = train,
                   method = 'rf',
                   metric = 'ROC',
                   tuneLength  = 15, 
                   trControl = rpartCtrl)
print(rf_random2)

```

Calculamos la curva ROC

```{r}
library(pROC)
predictionValidationProb <- predict(rf_random2, val, type = "prob")
auc2 <- roc(val$Survived,                       # columna 'target' del conjunto de entrenamiento
           predictionValidationProb[["Yes"]])  # columna de predicciones de la clase positiva como probabilidades
roc_validation2 <- plot.roc(auc2, 
                           ylim=c(0,1), 
                           type = "S" , 
                           print.thres = TRUE, 
                           main=paste('Validation AUC:', round(auc2$auc[[1]], 2)))

roc_validation1 <- plot.roc(auc1, 
                           ylim=c(0,1), 
                           type = "S" , 
                           print.thres = TRUE, 
                           main=paste('Validation AUC:', round(auc1$auc[[1]], 2)))
```


validacion:


```{r}
prediction <- predict(rf_random2, train, type = "raw") 
cm_train <- confusionMatrix(prediction, train[["Survived"]])
cm_train
```

## Red neural


```{r}
rpartCtrl <- trainControl(classProbs = TRUE)
trainIndex <- createDataPartition(data$Survived, p = .8, list = FALSE)
train <- data[trainIndex, ]
val <- data[-trainIndex, ]

#Aplicamos el metodo "mlp"
set.seed(1)
neumodelo_rf <- train(Survived ~ .,
                   data = train,
                   method = 'mlp',
                   metric = 'Accuracy',
                  trControl = rpartCtrl)
print(modelo_rf)

#Validacion
prediction <- predict(modelo_rf, val, type = "raw") 
cm_train <- confusionMatrix(prediction, val[["Survived"]])
cm_train

## Test del modelo
data_test_raw <- read_csv('test.csv')
test <- 
  data_test_raw %>%
  mutate(Pclass = as.factor(Pclass)) %>%
  mutate(Fare_Interval = as.factor(
    case_when(
      Fare >= 30 ~ 'More.than.30',
      Fare >= 20 & Fare < 30 ~ 'Between.20.30',
      Fare < 20 & Fare >= 10 ~ 'Between.10.20',
      Fare < 10 ~ 'Less.than.10',
      TRUE ~ 'Between.10.20')))
prediction <- predict(modelo_rf, test, type = "raw")
prediction_table <- 
  select(test, PassengerId) %>%
  mutate(Survived = 
           ifelse(prediction=='Yes', 1, 0)) 
   
write_csv(prediction_table, "Neural.csv")


```

##Mejora de red neuronal


```{r}
rpartCtrl <- trainControl(classProbs = TRUE)
rpartParametersGrid <- expand.grid(size = seq(from = 3, to = 3, by = 1), 
                                   decay = seq(from = 0.00001, to = 0.00005, by = 0.00001))
trainIndex <- createDataPartition(data$Survived, p = .8, list = FALSE)
train <- data[trainIndex, ]
val <- data[-trainIndex, ]

#Aplicamos el metodo "mlp"
set.seed(1)
modelo_rna_mejorado <- train(Survived ~ .,
                   data = train,
                   method = 'mlpWeightDecay',
                   metric = 'Accuracy',
                  trControl = rpartCtrl,
                  tuneGrid = rpartParametersGrid)
print(modelo_rna_mejorado)

#Validacion
prediction <- predict(modelo_rna_mejorado, val, type = "raw") 
cm_train <- confusionMatrix(prediction, val[["Survived"]])
cm_train

## Test del modelo
data_test_raw <- read_csv('test.csv')
test <- 
  data_test_raw %>%
  mutate(Pclass = as.factor(Pclass)) %>%
  mutate(Fare_Interval = as.factor(
    case_when(
      Fare >= 30 ~ 'More.than.30',
      Fare >= 20 & Fare < 30 ~ 'Between.20.30',
      Fare < 20 & Fare >= 10 ~ 'Between.10.20',
      Fare < 10 ~ 'Less.than.10',
      TRUE ~ 'Between.10.20')))
prediction <- predict(modelo_rna_mejorado, test, type = "raw")
prediction_table <- 
  select(test, PassengerId) %>%
  mutate(Survived = 
           ifelse(prediction=='Yes', 1, 0)) 
   
write_csv(prediction_table, "modelo_rna_mejorado.csv")


```

## Eleccion del mejor modelo

Tras realizar ambos modelos, el primero con los argumentos por defecto y el segundo variando los argumentos he llegado a la conclusión de que el mejor de ellos es el primero. Ya que modificando los parametros no he conseguido llegar al porcentaje de acierto del primer modelo.

De esta forma he conseguido en la competicion de kaggle un accuracy de 0,7799

















